# ğŸ©º Medicore-AI  
*A Retrieval-Augmented Medical Chat Assistant*  

## ğŸ” Introduction  
**Medicore-AI** is an intelligent medical chatbot designed to assist users in navigating complex medical literature. By leveraging **Retrieval-Augmented Generation (RAG)**, the system enhances large language models with relevant context extracted from uploaded documents such as medical textbooks, reports, or notes. This ensures that responses are not only fluent but also grounded in authoritative references, minimizing hallucinations and improving factual accuracy.  

---

## ğŸ§  How RAG Works  
Unlike a standard LLM that generates answers purely from its training data, RAG integrates an **external knowledge base**:  

1. **User Question** â†’ converted into embeddings  
2. **Vector Search** â†’ retrieves semantically similar chunks from a vector database  
3. **RAG Pipeline** â†’ combines query + retrieved context  
4. **LLM Response** â†’ grounded, context-aware medical answer  

This workflow allows the system to dynamically adapt to any new set of medical documents provided by the user.  

---

## âœ¨ Key Features  
- ğŸ“„ **PDF Uploads** â€“ Upload medical notes, research papers, or textbooks.  
- ğŸ” **Contextual Search** â€“ Automatic text extraction, chunking, and embedding.  
- ğŸ§¬ **Smart Embeddings** â€“ Powered by **Google/BGE models** for semantic precision.  
- ğŸ“¦ **Vector Database Integration** â€“ Uses **Pinecone** for fast retrieval.  
- âš¡ **LLM-Powered Answers** â€“ Built on **Groqâ€™s LLaMA3-70B** via **LangChain**.  
- ğŸŒ **Backend APIs** â€“ Built with **FastAPI** for efficient Q&A and file uploads.  
- ğŸ–¥ **Streamlit Frontend** â€“ Simple, interactive chat interface for users.  

---

## ğŸ›  Technology Stack  

| Component      | Technology Used |
|----------------|-----------------|
| **LLM**        | Groq API (LLaMA3-70B) |
| **Embeddings** | Google Generative AI / BGE |
| **Vector DB**  | Pinecone |
| **Framework**  | LangChain |
| **Backend**    | FastAPI |
| **Frontend**   | Streamlit |
| **Deployment** | Render |

---

## ğŸ“‚ Project Structure  
    Medicore-AI/
    â”œâ”€â”€ assets/ # Sample docs & assets
    â”œâ”€â”€ client/ # Streamlit frontend
    â”‚ â”œâ”€â”€ components/ # Chat UI, file upload, download history
    â”‚ â”œâ”€â”€ utils/ # API calls
    â”‚ â””â”€â”€ app.py
    â”œâ”€â”€ server/ # FastAPI backend
    â”‚ â”œâ”€â”€ middlewares/
    â”‚ â”œâ”€â”€ modules/ # LLM, vector store, handlers
    â”‚ â”œâ”€â”€ routes/ # API endpoints
    â”‚ â””â”€â”€ main.py
    â”œâ”€â”€ .gitignore
    â”œâ”€â”€ pyproject.toml
    â””â”€â”€ README.md
---

## ## ğŸ—ï¸ Architecture

![Medicore-AI Architecture]

## âš¡ Setup Instructions  

### 1ï¸âƒ£ Clone Repository  
    ```bash
    git clone https://github.com/SAISWARI-123/Medicore-AI.git
    cd Medicore-AI

2ï¸âƒ£ Backend Setup (FastAPI)
    
      cd server
      python -m venv venv
      venv\Scripts\activate  # (Linux/Mac: source venv/bin/activate)
      
      pip install -r requirements.txt
      
      Create a .env file in /server with:
      
      GOOGLE_API_KEY=your_google_key
      GROQ_API_KEY=your_groq_key
      PINECONE_API_KEY=your_pinecone_key
      Run the backend:
      uvicorn main:app --reload --port 8000

3ï¸âƒ£ Frontend Setup (Streamlit)
      
      cd client
      python -m venv venv
      venv\Scripts\activate  # (Linux/Mac: source venv/bin/activate)
      pip install -r requirements.txt
      streamlit run app.py

ğŸš€ Deployment

     Deploy on Render with the following start command:
     uvicorn main:app --host 0.0.0.0 --port 10000

ğŸ™Œ Credits
    Developed by B. Saiswari Patro
    Inspired by cutting-edge ecosystems: LangChain, Groq, Pinecone, FastAPI

ğŸ“œ License
  This project is licensed under the MIT License â€“ free to use and modify with attribution.
